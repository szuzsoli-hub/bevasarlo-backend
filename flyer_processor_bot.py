import os
import time
import json
import re
import requests # <-- √öJ IMPORT
import fitz # <-- √öJ IMPORT (PyMuPDF a PDF szeletel√©shez)
from dotenv import load_dotenv
from openai import OpenAI
from google.cloud import vision
import datetime

# Selenium importok
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.action_chains import ActionChains

# ==============================
# 0. KONFIGUR√ÅCI√ì & ENV
# ==============================

base_dir = os.path.dirname(os.path.abspath(__file__))
load_dotenv(os.path.join(base_dir, ".env"))

# --- √öJ: ASSETS MAPPA KEZEL√âSE ---
ASSETS_DIR = os.path.join(base_dir, "assets")
if not os.path.exists(ASSETS_DIR):
    os.makedirs(ASSETS_DIR)

# Mindk√©t f√°jlt az assets mapp√°n bel√ºl kezelj√ºk!
INPUT_FILE = os.path.join(ASSETS_DIR, 'flyers.json')           # A friss linkek
OUTPUT_FILE = os.path.join(ASSETS_DIR, 'universal_output.json') # A k√©sz adatb√°zis
# GitHub Actions k√∂rnyezetben a secretb≈ël j√∂n, lok√°lisan a f√°jlb√≥l/env-b≈ël
if not os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "google_key.json"

openai_key = os.getenv("OPENAI_API_KEY")

if not openai_key:
    print("‚ö†Ô∏è FIGYELEM: Nincs OpenAI kulcs a k√∂rnyezeti v√°ltoz√≥kban!")

client = OpenAI(api_key=openai_key)
vision_client = vision.ImageAnnotatorClient()

TEMP_DIR = os.path.join(base_dir, "temp_kepek")
if not os.path.exists(TEMP_DIR):
    os.makedirs(TEMP_DIR)


# ===============================================================================
# 1/A. MODUL: A FOT√ìS (Capture - HTML/Selenium) üì∏
# ===============================================================================

def capture_pages_with_selenium(target_url, store_name):
    print(f"\nüì∏ FOT√ìZ√ÅS INDUL ({store_name}): {target_url}")

    chrome_options = Options()
    chrome_options.add_argument("--headless") # GitHub Actions miatt k√∂telez≈ë!
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    
    # --- M√ìDOS√çT√ÅS: Szafari √°lc√°z√°s √©s anti-bot v√©delem a Spar miatt ---
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 14_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.4 Safari/605.1.15")

    captured_data = []

    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        # Extr√©m bot elrejt√©s JavaScripttel
        driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
            "source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
        })
        
        driver.get(target_url)
        
        # --- M√ìDOS√çT√ÅS: 10 m√°sodperc univerz√°lis bet√∂lt√©si id≈ë (HD k√©pek √©s √©les d√°tumok miatt) ---
        time.sleep(10) 

        # S√úTI KEZEL√âS
        try:
            buttons = driver.find_elements(By.TAG_NAME, "button")
            for btn in buttons:
                txt = btn.text.lower()
                if any(x in txt for x in ["elfogad", "accept", "mindent", "ok", "rendben"]):
                    driver.execute_script("arguments[0].click();", btn)
                    time.sleep(1)
                    break
        except:
            pass
        
        # Zavar√≥ elemek t√∂rl√©se
        try:
            driver.execute_script("""
                document.querySelectorAll('div[class*="cookie"], div[id*="cookie"], #onetrust-banner-sdk').forEach(el => el.remove());
            """)
        except:
            pass

        # --- M√ìDOS√çT√ÅS: 4 oldalra felemelve a teszt kedv√©√©rt ---
        for i in range(4): 
            page_num = i + 1
            fajl_nev = os.path.join(TEMP_DIR, f"{store_name}_oldal_{page_num}.png")
            
            # --- √öJ M√ìDOS√çT√ÅS: Lapoz√°s Iframe-en bel√ºl a jobbra ny√≠llal ---
            if i > 0:
                try:
                    iframes = driver.find_elements(By.TAG_NAME, "iframe")
                    if iframes:
                        # Ha van iframe (pl. Spar flipbook), bel√©p√ºnk √©s oda k√ºldj√ºk a nyilat
                        driver.switch_to.frame(iframes[0])
                        body = driver.find_element(By.TAG_NAME, 'body')
                        body.send_keys(Keys.ARROW_RIGHT)
                        driver.switch_to.default_content() # Visszal√©p√ºnk a f≈ëoldalra a fot√≥z√°shoz
                    else:
                        # Sima oldal eset√©n marad a norm√°l lapoz√°s
                        body = driver.find_element(By.TAG_NAME, 'body')
                        body.send_keys(Keys.ARROW_RIGHT)
                except Exception as e:
                    print(f"‚ö†Ô∏è Lapoz√°si hiba: {e}")
                
                # --- M√ìDOS√çT√ÅS: 5 m√°sodperc v√°rakoz√°s lapoz√°s ut√°n a HD k√©p bet√∂lt√©s√©hez ---
                time.sleep(5)

            # Visszat√©r√©s a biztons√°gos, teljes k√©perny≈ës fot√≥z√°shoz
            driver.save_screenshot(fajl_nev)

            captured_data.append({
                "image_path": fajl_nev,
                "page_url": driver.current_url,
                "page_num": page_num
            })
            print(f"   -> {page_num}. oldal lefot√≥zva. (URL: {driver.current_url})")

        return captured_data

    except Exception as e:
        print(f"‚ùå Hiba a fot√≥z√°sn√°l ({store_name}): {e}")
        return []
    finally:
        if 'driver' in locals(): driver.quit()


# ===============================================================================
# 1/B. MODUL: A SZELETEL≈ê (PDF Let√∂lt√©s √©s darabol√°s) ‚úÇÔ∏èüìÑ
# ===============================================================================

def capture_pages_from_pdf(target_url, store_name):
    print(f"\nüìÑ PDF LET√ñLT√âS √âS SZELETEL√âS INDUL ({store_name}): {target_url}")
    captured_data = []
    temp_pdf_path = os.path.join(TEMP_DIR, f"{store_name}_temp.pdf")

    # --- M√ìDOS√çT√ÅS: Safari √°lca (headers) a 403-as hiba ellen ---
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.4 Safari/605.1.15"
    }

    try:
        # 1. Nyers PDF f√°jl let√∂lt√©se Safari √°lc√°val
        response = requests.get(target_url, headers=headers, timeout=30)
        response.raise_for_status()
        with open(temp_pdf_path, 'wb') as f:
            f.write(response.content)

        # 2. PDF megnyit√°sa √©s darabol√°sa (PyMuPDF)
        doc = fitz.open(temp_pdf_path)
        max_pages = min(4, len(doc)) # Maximum 4 oldal

        for i in range(max_pages):
            page_num = i + 1
            page = doc.load_page(i)
            # K√©p gener√°l√°sa (dpi=200 a t√∂k√©letes, t≈±√©les OCR-hez)
            pix = page.get_pixmap(dpi=200)
            fajl_nev = os.path.join(TEMP_DIR, f"{store_name}_oldal_{page_num}.png")
            pix.save(fajl_nev)

            # --- Deep Link horgonnyal a pontos oldalhoz ---
            captured_data.append({
                "image_path": fajl_nev,
                "page_url": f"{target_url}#page={page_num}",
                "page_num": page_num
            })
            print(f"   -> {page_num}. oldal t√∂k√©letes min≈ës√©gben kiv√°gva a PDF-b≈ël.")

        doc.close()
        return captured_data

    except Exception as e:
        print(f"‚ùå Hiba a PDF feldolgoz√°s√°n√°l ({store_name}): {e}")
        return []
    finally:
        # Takar√≠t√°s: A let√∂lt√∂tt nyers PDF-et azonnal eldobjuk
        if os.path.exists(temp_pdf_path):
            try:
                os.remove(temp_pdf_path)
            except:
                pass


# ===============================================================================
# 2. MODUL: AZ AGY - D√ÅTUM ELLEN≈êRZ√âS √âS AI OSZT√ÅLYOZ√ÅS (BOUNCER) üß†
# ===============================================================================

def google_ocr(image_path):
    with open(image_path, "rb") as img_file: content = img_file.read()
    image = vision.Image(content=content)
    response = vision_client.document_text_detection(image=image)
    if response.error.message: return ""
    return response.full_text_annotation.text

def interpret_text_with_ai(full_text, page_num, store_name, title_name):
    # D√°tum instrukci√≥ csak az els≈ë oldalon
    date_instr = "FELADAT 1: KERESD MEG AZ AKTU√ÅLIS √âRV√âNYESS√âGI ID≈êT (YYYY.MM.DD-YYYY.MM.DD) a sz√∂vegben! Keresd ki az √∂sszes d√°tumot, amit l√°tsz!" if page_num == 1 else ""

    # --- M√ìDOS√çT√ÅS: AI T√©rf√©l felismer≈ë be√©p√≠t√©se ---
    prompt = f"""
    Kapt√°l egy OCR sz√∂veget a(z) {store_name} bolt "{title_name}" √∫js√°gj√°nak {page_num}. oldal√°r√≥l.
    FIGYELEM: Ez a k√©p gyakran egy dupla oldalp√°rt (pl. 6-7. oldal) √°br√°zol!
    {date_instr}

    FELADAT 2: KATEGORIZ√ÅL√ÅS (Azonos√≠tsd az oldal f≈ë profilj√°t!)
    - Ha t√∫lnyom√≥r√©szt √©lelmiszer, ital, napi fogyaszt√°si cikk vagy h√°ztart√°si vegyi √°ru van rajta -> "√âLELMISZER_VEGYES"
    - Ha tiszt√°n ruha, bark√°cs, b√∫tor, elektronika, vagy im√°zs/√°ll√°shirdet√©s konkr√©t term√©k n√©lk√ºl -> "NONFOOD_MARKETING"

    FELADAT 3: TERM√âKEK KIGY≈∞JT√âSE (Csak ha az oldal √âLELMISZER_VEGYES!)
    Gy≈±jtsd ki az √©lelmiszer √©s vegyi √°ru term√©keket JSON-be. 
    (Ha az oldal NONFOOD_MARKETING, a 'termekek' lista maradjon √ºresen: []).

    MEZ≈êK √âS FORM√ÅTUMOK:
    - 'nev': Term√©k neve.
    - 'ar': √År. Ez a fizetend≈ë TELJES √°r legyen (pl. a csomag √°ra)! K√ñTELEZ≈ê FORM√ÅTUM: A sz√°m ut√°n mindig √≠rd oda a valut√°t is! (pl. "999 Ft", "229 Ft/db", vagy "4699 Ft"). SOHA ne az egys√©g√°rat tedd ide!
    - 'ar_info': Kiszerel√©s √âS egys√©g√°r. T√ñREKEDJ ERRE AZ ETALON FORM√ÅTUMRA: [Mennyis√©g], [Egys√©g√°r]. P√©ld√°k: "500 g, 1398 Ft/kg", vagy "40 db, 117,5 Ft/db", vagy "1.5 l, 499 Ft/l". KIV√âTEL: Ha valamelyik adat hi√°nyzik a k√©pr≈ël vagy olvashatatlan, NE dobd el a term√©ket, csak azt √≠rd be, amit biztosan l√°tsz!
    - 'ar_info2': Felt√©telek (pl. "Csak 2 db eset√©n", "Clubcarddal"). Ha nincs, legyen null.
    - 'oldal_terfel': Hat√°rozd meg, hogy a term√©k a k√©p BAL vagy JOBB t√©rfel√©n tal√°lhat√≥-e. Ha a k√©p csak egyetlen oldalt √°br√°zol, akkor legyen "bal". √ârt√©ke csak "bal" vagy "jobb" lehet.

    ELV√ÅRT JSON FORMAT:
    {{
      "oldal_jelleg": "√âLELMISZER_VEGYES",
      "ervenyesseg": "2026.02.12-2026.02.18", 
      "termekek": [
        {{ "nev": "...", "ar": "999 Ft", "ar_info": "500 g, 1398 Ft/kg", "ar_info2": null, "oldal_terfel": "jobb", "kategoria_dontes": "marad" }}
      ]
    }}
    
    OCR SZ√ñVEG:
    {full_text}
    """

    response = client.chat.completions.create(
        model="gpt-4o",
        temperature=0,
        response_format={"type": "json_object"},
        messages=[{"role": "user", "content": prompt}],
    )
    return json.loads(response.choices[0].message.content)

def check_validity_date(date_string):
    """
    K√∂zponti D√°tum Ellen≈ër.
    True = √ârv√©nyes
    False = Lej√°rt (Azonnali t√∂rl√©s)
    """
    if not date_string: return True # Ha nincs adat, a biztons√°g kedv√©√©rt √°tengedj√ºk (User check)
    
    try:
        # D√°tum keres√©s (YYYY.MM.DD vagy YYYY-MM-DD)
        dates = re.findall(r'\d{4}[\.\-]\d{2}[\.\-]\d{2}', str(date_string))
        
        if dates:
            dates.sort()
            
            # Az utols√≥ (legk√©s≈ëbbi) d√°tum a lej√°rati id≈ë
            end_date_str = dates[-1].replace('-', '.')
            end_date = datetime.datetime.strptime(end_date_str, "%Y.%m.%d").date()
            today = datetime.date.today()
            
            if end_date < today:
                return False # LEJ√ÅRT
            else:
                return True # M√âG J√ì
                
    except Exception:
        pass 
        
    return True

def process_images_with_ai(captured_data, flyer_meta):
    print(f"üß† AI Elemz√©s: {flyer_meta['store']} - {flyer_meta['title']}...")
    results = []
    detected_validity = flyer_meta.get('validity', "N/A")
    nonfood_count = 0

    try:
        for item in captured_data:
            full_text = google_ocr(item['image_path'])
            if not full_text: 
                continue

            # √Åtadjuk a bolt √©s √∫js√°g nevet a promptnak, hogy az AI-nak ne kelljen kital√°lnia
            structured = interpret_text_with_ai(full_text, item['page_num'], flyer_meta['store'], flyer_meta['title'])

            # --- 1. BOUNCER: FRISS √öJS√ÅG D√ÅTUM ELLEN≈êRZ√âS ---
            if item['page_num'] == 1:
                is_valid = True
                
                # A Hibrid Nyomoz√≥ (Spar Specifikus)
                if "spar" in flyer_meta['store'].lower():
                    url_date_match = re.search(r'(2[4-6])(0[1-9]|1[0-2])(0[1-9]|[12]\d|3[01])', flyer_meta['url'])
                    # A nyers OCR sz√∂vegb≈ël (full_text) keress√ºk a d√°tumokat, nem az AI-t√≥l!
                    ocr_detected_dates = re.findall(r'\d{4}[\.\-]\d{2}[\.\-]\d{2}', full_text)
                    
                    found_exact_match = False
                    
                    if url_date_match and len(ocr_detected_dates) >= 2:
                        # Ha van d√°tum az URL-ben, kinyerj√ºk (pl. 260219 -> 2026.02.19)
                        y, m, d = url_date_match.groups()
                        expected_start = f"20{y}.{m}.{d}"
                        
                        # Megn√©zz√ºk a NYERS OCR √°ltal tal√°lt d√°tumokat p√°ros√°val (kezdet-v√©g)
                        for i in range(0, len(ocr_detected_dates)-1, 2):
                            start_date = ocr_detected_dates[i].replace('-', '.')
                            end_date = ocr_detected_dates[i+1].replace('-', '.')
                            
                            if start_date == expected_start:
                                detected_validity = f"{start_date}-{end_date}"
                                found_exact_match = True
                                is_valid = check_validity_date(detected_validity)
                                print(f"üéØ HIBRID NYOMOZ√ì SIKER: Megvan a pontos Spar d√°tum: {detected_validity}")
                                break
                    
                    # A MENT≈ê√ñV: Ha a Hibrid Nyomoz√≥ elbukott, B√çZZUNK A LINKVAD√ÅSZBAN!
                    if not found_exact_match:
                        print("üõ°Ô∏è SPAR V√âD≈êH√ÅL√ì: Nincs biztos OCR d√°tum, de √°tengedj√ºk a Linkvad√°sz frissess√©ge alapj√°n!")
                        detected_validity = flyer_meta.get('validity', "N/A")
                        is_valid = True # √Åtengedj√ºk!
                
                # Ha NEM Spar, marad a r√©gi ellen≈ërz√©s
                else:
                    if structured.get("ervenyesseg"):
                        detected_validity = structured.get("ervenyesseg")
                        is_valid = check_validity_date(detected_validity)

                # Ha a d√°tum garant√°ltan lej√°rt -> KUKA
                if not is_valid:
                     print(f"‚õî BOUNCER: Ez az √∫js√°g lej√°rt ({detected_validity}), teljes t√∂rl√©s! - {flyer_meta['title']}")
                     return [] # Megszak√≠tja az AI elemz√©st

            # --- 2. BOUNCER: NONFOOD / MARKETING SZ≈∞R≈ê ---
            jelleg = structured.get("oldal_jelleg", "√âLELMISZER_VEGYES")
            if jelleg == "NONFOOD_MARKETING":
                print(f"   ‚è© SKIP: A(z) {item['page_num']}. oldal '{jelleg}' besorol√°st kapott.")
                nonfood_count += 1
                
                # --- M√ìDOS√çT√ÅS: 2 oldal helyett az els≈ë 3 oldal ut√°n dobja csak ki (Spar Extra miatt) ---
                if item['page_num'] == 3 and nonfood_count == 3:
                    print(f"‚õî BOUNCER: Az els≈ë 3 oldal NONFOOD. Eg√©sz √∫js√°g kuka! - {flyer_meta['title']}")
                    return []
                continue # √Åtugorja a term√©kek list√°z√°s√°t ezen az oldalon

            # --- TERM√âKEK KIMENT√âSE (Prec√≠z Deep Linkkel √©s k√©sz metaadatokkal) ---
            for product in structured.get("termekek", []):
                if product.get("kategoria_dontes") == "marad":
                    
                    # === √öJ: OLDAL T√âRF√âL (BAL/JOBB) MATEK ===
                    terfel = product.get("oldal_terfel", "bal").lower()
                    vegleges_link = item['page_url']
                    vegleges_oldalszam = item['page_num']
                    
                    # Ha a jobb oldalon van (√âS AZ EREDETI FORR√ÅS NEM PDF), a linket √âS az oldalsz√°mot is megn√∂velj√ºk eggyel!
                    if terfel == "jobb" and not flyer_meta['url'].lower().endswith('.pdf'):
                        vegleges_link = re.sub(r'(\d+)(/?)$', lambda m: str(int(m.group(1)) + 1) + m.group(2), item['page_url'])
                        vegleges_oldalszam = item['page_num'] + 1
                    
                    record = {
                        "bolt": flyer_meta['store'],
                        "ujsag": flyer_meta['title'],
                        "oldalszam": vegleges_oldalszam,  # <--- MOST M√ÅR A KI√çRT SZ√ÅM IS PONTOS LESZ!
                        "ervenyesseg": detected_validity,
                        "nev": product.get("nev"),
                        "ar": product.get("ar"),
                        "ar_info": product.get("ar_info"),
                        "ar_info2": product.get("ar_info2"),
                        "forrasLink": vegleges_link, # A Jogi v√©delemhez (Most m√°r kicentizve!)
                        "alap_link": flyer_meta['url']  # A deduplik√°ci√≥hoz √©s j√∂v≈ëbeli csekkol√°shoz
                    }
                    results.append(record)
                    print(f"      + {record['nev']} | {record['ar']} | T√©rf√©l: {terfel.upper()}")

    except Exception as e:
        print(f"‚ö†Ô∏è Hiba az AI feldolgoz√°sn√°l: {e}")
    finally:
        # --- BIZTONS√ÅGI TAKAR√çT√ÅS (SZIV√ÅRG√ÅSMENTES√çT√âS) ---
        # Ez mindenk√©pp lefut, ha siker√ºlt, ha hib√°ra futott, ha a Bouncer kidobta az √∫js√°got!
        for item in captured_data:
            if os.path.exists(item['image_path']):
                try:
                    os.remove(item['image_path'])
                except Exception:
                    pass
        print(f"üßπ Takar√≠t√°s: A(z) {flyer_meta['store']} √°tmeneti k√©pei marad√©ktalanul t√∂r√∂lve lettek.")

    return results


# ===============================================================================
# F≈êVEZ√âRL≈ê (TISZT√çT√ÅS + BOUNCER + DEDUPLIK√ÅCI√ì) üßπ‚õîüí∞
# ===============================================================================

if __name__ == "__main__":
    print("=== PROFESSZOR BOT: TOTAL CLEANUP VERZI√ì (v6.2 - PDF Szeletel≈ëvel) ===")
    print(f"üìÖ Mai d√°tum: {datetime.date.today()}")

    # 1. Friss linkek bet√∂lt√©se (Ez a referencia!)
    if not os.path.exists(INPUT_FILE):
        print("‚ùå Nincs flyers.json! Futtasd a Linkvad√°szt el≈ëbb.")
        exit()
    
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        new_flyers_data = json.load(f)
        current_flyers = new_flyers_data.get("flyers", [])
        
    current_active_urls = [f['url'] for f in current_flyers]
    print(f"üìã Akt√≠v √∫js√°gok linkjei (Web): {len(current_active_urls)}")

    # 2. R√©gi adatok bet√∂lt√©se
    old_products = []
    if os.path.exists(OUTPUT_FILE):
        try:
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                old_products = json.load(f)
        except:
            old_products = []

    # 3. K√âT-L√âPCS≈êS TISZT√çT√ÅS (R√âGI ADATOK SZ≈∞R√âSE)
    final_products = []
    kept_count = 0
    dropped_link = 0
    dropped_date = 0
    
    print("‚ôªÔ∏è  R√©gi adatok ellen≈ërz√©se...")
    for product in old_products:
        # Itt az 'alap_link'-et n√©zz√ºk, ha m√°r l√©tezik (√∫j form√°tum), de t√°mogatjuk a r√©git is ('forrasLink')
        p_base_link = product.get('alap_link', product.get('forrasLink'))
        p_date = product.get('ervenyesseg')
        
        # A) Link ellen≈ërz√©s: M√©g kint van a boltn√°l?
        if p_base_link not in current_active_urls:
            dropped_link += 1
            continue # T√∂r√∂lj√ºk, mert a bolt levette a linket
            
        # B) D√°tum ellen≈ërz√©s: A JSON-ban t√°rolt d√°tum lej√°rt-e m√°ra?
        if not check_validity_date(p_date):
            dropped_date += 1
            continue # T√∂r√∂lj√ºk, mert lej√°rt az ideje
            
        # Ha mindkett≈ën √°tment -> MEGTARTJUK
        final_products.append(product)
        kept_count += 1

    print(f"   -> Megtartva: {kept_count}")
    print(f"   -> T√∂r√∂lve (Hib√°s link): {dropped_link}")
    print(f"   -> T√∂r√∂lve (Lej√°rt d√°tum): {dropped_date}")
    
    # Jegyezz√ºk meg, miket tartottunk meg (URL alapj√°n), hogy ne dolgozzuk fel √∫jra
    processed_urls_in_output = set()
    for p in final_products:
        p_base_link = p.get('alap_link', p.get('forrasLink'))
        processed_urls_in_output.add(p_base_link)

    # 4. √öJ LINKKEK FELDOLGOZ√ÅSA (BOUNCER M√ìD)
    for flyer in current_flyers:
        url = flyer['url']
        
        # DEDUPLIK√ÅCI√ì: Ha m√°r megvan a tiszt√≠tott list√°ban -> SKIP
        if url in processed_urls_in_output:
            print(f"‚è© SKIP (√ârv√©nyes √©s k√©sz): {flyer['store']} - {flyer['title']}")
            continue 
            
        # HA √öJ -> FELDOLGOZ√ÅS INDUL
        print(f"\nüÜï √öJ √öJS√ÅG! Vizsg√°lat indul: {flyer['store']} - {flyer['title']}")

        # --- AZ √öTV√ÅLASZT√ì (KAPU≈êR) ---
        if url.lower().endswith('.pdf'):
            pages = capture_pages_from_pdf(url, flyer['store'])
        else:
            pages = capture_pages_with_selenium(url, flyer['store'])
        
        if pages:
            # Itt fut le a BOUNCER (process_images_with_ai).
            # Ha az AI szerint lej√°rt, vagy NONFOOD a katal√≥gus, √ºres list√°t ad vissza.
            new_items = process_images_with_ai(pages, flyer)
            
            if new_items:
                final_products.extend(new_items)
                print(f"‚úÖ SIKER! {len(new_items)} db term√©k hozz√°adva.")
            else:
                print("üö´ BLOKKOLVA (Lej√°rt √∫js√°g vagy teljesen Non-Food katal√≥gus).")
        else:
            print("‚ö†Ô∏è Nem siker√ºlt a fot√≥z√°s.")

    # 5. V√âGS≈ê MENT√âS
    # Itt fel√ºl√≠rjuk a f√°jlt a tiszt√≠tott + √∫j list√°val
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(final_products, f, ensure_ascii=False, indent=2)

    print(f"\nüèÅ K√âSZ! V√©gs≈ë adatb√°zis: {len(final_products)} term√©k.")



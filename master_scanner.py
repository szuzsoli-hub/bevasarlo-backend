import requests
from curl_cffi import requests as cffi_requests
from bs4 import BeautifulSoup
import re
import datetime
import json
import os
import time
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# K√úLS≈ê MODUL IMPORT√ÅL√ÅSA
try:
    from spar_hunter import scan_spar_only
except ImportError:
    print("‚ö†Ô∏è FIGYELEM: Nem tal√°lom a spar_hunter.py f√°jlt! A SPAR szkennel√©s kimarad.")
    def scan_spar_only(): return []

# --- KONFIGUR√ÅCI√ì ---
OUTPUT_FILE = 'flyers.json'


# ===============================================================================
# 1. R√âSZ: HAGYOM√ÅNYOS BOLTOK (Protocol & Scanners)
# ===============================================================================

def analyze_link(store_name, title, url):
    t = title.lower()
    u = url.lower()

    # --- 1. PENNY ---
    if store_name == "Penny":
        if "eletmod" in u or "√©letm√≥d" in t or "recept" in t:
            return "DROP"

    # --- 2. AUCHAN (Most m√°r a non-food sz≈±r≈ëvel!) ---
    elif store_name == "Auchan":
        if any(x in u for x in ["bizalom", "qilive", "textil", "jatek", "kert", "auto", "adatvedelem", "tajekoztato", "nonfood", "m≈±szaki", "elektronika"]):
            return "DROP"
        if any(x in t for x in ["nonfood", "m≈±szaki", "elektronika"]):
            return "DROP"

    # --- 3. LIDL ---
    elif store_name == "Lidl":
        if "parkside" in u or "barkacs" in u or "nonfood" in u or "non-food" in u:
            return "DROP"
        if any(x in t for x in ["szabadid≈ë", "utaz√°s", "recept", "bark√°cs"]):
            return "DROP"

    # --- 4. ALDI ---
    elif store_name == "Aldi":
        if any(x in t for x in ["utaz√°s", "k√∂z√©ps≈ë sor", "kert", "vir√°g"]):
            return "DROP"

    # --- 5. TESCO ---
    elif store_name == "Tesco":
        if any(x in t for x in ["kerti", "j√°t√©k", "ruha", "f&f", "mobile"]):
            return "DROP"

    # --- 6. SPAR ---
    elif store_name == "Spar":
        if "lifestyle" in t:
            return "DROP"

    # --- 7. METRO (Szigor√≠tott Nagyker Sz≈±r≈ë) ---
    elif store_name == "Metro":
        blacklist_words = [
            "kiskeresked", "k√°v√©", "√°zsiai", "olasz", "szt√°r",
            "nagyobb mennyis√©g", "kantin", "professzion√°lis",
            "street food", "gasztro", "iroda", "vend√©gl√°t√≥",
            "horeca", "g√©p", "b√∫tor", "konyha", "sirha"
        ]
        if any(b in t for b in blacklist_words):
            return "DROP"
        return "KEEP"

    return "KEEP"


# --- √öJ: C√çM GENER√ÅL√ì F√úGGV√âNY (SLUG ALAPJ√ÅN) ---
def get_slug_title(store, current_title, url):
    """
    A k√©rt boltokn√°l lecser√©li a c√≠met a link utols√≥, besz√©des r√©sz√©re (slug).
    """
    final_title = current_title
    
    # --- AUCHAN: Link v√©ge (pl. .../2026-02-19-04-04-husveti-ajanlataink) ---
    if store == "Auchan":
        slug = url.split('/')[-1] # A link utols√≥ r√©sze a perjel ut√°n
        if slug:
            final_title = slug # C√≠mnek adjuk a slug-ot

    # --- ALDI: Link v√©ge (pl. .../online_akcios_ujsag_2026_02_19_kw08_psf9y3ck) ---
    elif store == "Aldi":
        slug = url.split('/')[-1]
        if slug:
            # Regex: megkeresi a '_kw' + sz√°mok + '_' + v√©letlenszer≈± karakterek r√©szt a string v√©g√©n, √©s lev√°gja
            clean_slug = re.sub(r'_kw\d+_[a-zA-Z0-9]+$', '', slug)
            final_title = clean_slug

    # --- SPAR: Link v√©ge a 'spar/' vagy 'interspar/' stb. ut√°n ---
    # A SPAR Hunter m√°r eleve az URL-b≈ël gener√°lt c√≠met adhat vissza, 
    # de itt a Master Scannerben is r√°er≈ës√≠thet√ºnk, ha a list√°t b≈ëv√≠tj√ºk.
    # (Megjegyz√©s: A scan_spar() f√ºggv√©ny lentebb kezeli a beh√≠v√°st, 
    # de ha a be√©rkez≈ë adat c√≠m√©t m√≥dos√≠tani akarjuk, azt ott kell majd.)

    return final_title


def scan_metro():
    print("\n--- METRO Szkennel√©s (Szigor√≠tott) ---")
    url = "https://cdn.metro-online.com/api/catalog-filter?resolution=600&feeds=metro-nagykereskedelem&collection_id=6365&metatags%5B%5D=channel=website"
    found = []
    try:
        response = requests.get(url, timeout=10)
        data = response.json()
        if 'items' in data:
            for item in data['items']:
                raw_title = item.get('name', 'Metro Katal√≥gus')
                link = item.get('url', '')
                if not link: continue
                status = analyze_link("Metro", raw_title, link)
                if status == "KEEP":
                    print(f"[{status}] {raw_title} -> {link}")
                    # VALIDITY T√ñR√ñLVE
                    found.append({"store": "Metro", "title": raw_title, "url": link})
    except Exception as e:
        print(f"‚ùå Metro Hiba: {e}")
    return found


def scan_spar():
    print("\n--- SPAR Szkennel√©s (K√ºls≈ë modul: spar_hunter.py) ---")
    try:
        found_raw = scan_spar_only()
        found_processed = []
        
        # ITT VAL√ìS√çTJUK MEG A SPAR C√çM CSER√âT (URL SLUG ALAPJ√ÅN)
        for item in found_raw:
            url = item['url']
            # Kiv√°gjuk az utols√≥ r√©szt: pl. '260212-1-spar-szorolap'
            slug = url.rstrip('/').split('/')[-1]
            
            # Friss√≠tj√ºk a c√≠met a slug-ra
            item['title'] = slug
            # VALIDITY T√ñR√ñLVE (Biztos√≠t√©k, ha a k√ºls≈ë modul beletette volna)
            item.pop('validity', None)

            found_processed.append(item)
            
        print(f"‚úÖ Master Scanner √°tvette √©s √°tnevezte a SPAR adatokat: {len(found_processed)} db")
        return found_processed
    except Exception as e:
        print(f"‚ùå Hiba a k√ºls≈ë SPAR modul futtat√°sakor: {e}")
        return []


def scan_auchan():
    print("\n--- AUCHAN Szkennel√©s (Selenium-alap√∫ 'J√∂v≈ë heti' t√°mogat√°ssal) ---")
    url = "https://www.auchan.hu/katalogusok"
    found = []
    
    opts = Options()
    opts.add_argument("--headless")
    opts.add_argument("--window-size=1920,1080")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)
    wait = WebDriverWait(driver, 15)

    try:
        driver.get(url)
        time.sleep(3)

        try:
            cookie_btn = wait.until(EC.element_to_be_clickable((By.ID, "onetrust-accept-btn-handler")))
            cookie_btn.click()
        except: pass

        source_aktualis = driver.page_source

        print("üîé 'J√∂v≈ë heti katal√≥gusok' f√ºl aktiv√°l√°sa...")
        try:
            next_btns = driver.find_elements(By.XPATH, "//*[contains(text(), 'J√∂v≈ë heti katal√≥gusok')]")
            clicked = False
            for btn in next_btns:
                try:
                    driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", btn)
                    time.sleep(1)
                    driver.execute_script("arguments[0].click();", btn)
                    clicked = True
                    print("‚úÖ J√∂v≈ë heti f√ºl akt√≠v.")
                    break
                except: continue
            
            if clicked:
                time.sleep(4) 
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight/2);")
                time.sleep(2)
                source_jovoheti = driver.page_source
            else:
                source_jovoheti = ""
        except:
            source_jovoheti = ""

        full_text = (source_aktualis + source_jovoheti).replace(r'\/', '/')
        found_raw_links = set()
        found_raw_links.update(re.findall(r'(https?://reklamujsag\.auchan\.hu/online-katalogusok/[^"\'\s<>]+)', full_text))
        for m in re.findall(r'(/online-katalogusok/[^"\'\s<>]+)', full_text):
            found_raw_links.add(f"https://reklamujsag.auchan.hu{m}")

        seen_links = set()
        for full_link in found_raw_links:
            full_link = full_link.rstrip('/').rstrip("'").rstrip('"').split('?')[0]
            if full_link in seen_links: continue

            # --- EREDETI C√çM GENER√ÅL√ÅS (CSAK LOGOL√ÅSHOZ) ---
            slug = full_link.split('/')[-1]
            title_match = re.search(r'\d{4}-\d{2}-\d{2}-(.+)', slug)
            clean_title = title_match.group(1).replace('-', ' ').title() if title_match else slug.replace('-', ' ').title()
            title = f"Auchan {clean_title}"

            status = analyze_link("Auchan", title, full_link)
            if status == "KEEP":
                # --- √öJ: C√çM CSER√âJE A LINK V√âG√âRE (SLUG) ---
                better_title = get_slug_title("Auchan", title, full_link)
                
                print(f"[{status}] {better_title} -> {full_link}")
                # VALIDITY T√ñR√ñLVE
                found.append({"store": "Auchan", "title": better_title, "url": full_link})
                seen_links.add(full_link)

    except Exception as e:
        print(f"‚ùå Auchan Hiba: {e}")
    finally:
        driver.quit()
    return found


def scan_penny():
    print("\n--- PENNY Szkennel√©s ---")
    url = "https://www.penny.hu/reklamujsag"
    headers = {'User-Agent': 'Mozilla/5.0'}
    found = []
    processed_ids = set()
    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        script = soup.find('script', id='__NUXT_DATA__')
        if script:
            matches = re.findall(r'https:[^"\'\s]*rewe\.co\.at[^"\'\s]*', script.string)
            for raw_link in matches:
                clean_link = raw_link.replace('\\u002F', '/').replace('\\/', '/')
                if '"' in clean_link: clean_link = clean_link.split('"')[0]
                
                base_url = clean_link.split('?')[0]
                if base_url not in processed_ids and ".jpg" not in base_url:
                    processed_ids.add(base_url)
                    
                    # --- √öJ: PENNY URL ALAP√ö C√çMGENER√ÅL√ÅS ---
                    title = "Penny Akci√≥s √öjs√°g"
                    if "eletmod" in clean_link:
                        title = "Penny √âletm√≥d"
                    else:
                        # Kinyerj√ºk a 202608 form√°tumot
                        match = re.search(r'/(\d{4})(\d{2})/', clean_link)
                        if match:
                            year, week = match.groups()
                            title = f"Penny Akci√≥s √öjs√°g {int(week)}. heti ({year}{week})"
                    
                    status = analyze_link("Penny", title, clean_link)
                    if status == "KEEP":
                        print(f"[{status}] {title} -> {clean_link}")
                        # VALIDITY T√ñR√ñLVE
                        found.append({"store": "Penny", "title": title, "url": clean_link})
    except Exception as e:
        print(f"‚ùå Penny Hiba: {e}")
    return found


def scan_lidl():
    print("\n--- LIDL Szkennel√©s (Vissza√°ll√≠tva Requests-re) ---")
    url = "https://www.lidl.hu/c/szorolap/s10013623"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'}
    seen = set()
    found = []
    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        for flyer in soup.find_all('a', class_='flyer'):
            link = flyer.get('href')
            if not link: continue
            if not link.startswith('http'): link = f"https://www.lidl.hu{link}"
            
            raw_title = flyer.find(class_='flyer__title')
            
            # --- √öJ: LIDL URL ALAP√ö C√çMGENER√ÅL√ÅS ---
            match = re.search(r'/ujsag/([^/]+)', link)
            if match:
                slug = match.group(1)
                # Elt√°vol√≠tjuk a v√©g√©r≈ël a '-2026' vagy hasonl√≥ √©vsz√°mot ha van
                slug = re.sub(r'-\d{4}$', '', slug)
                title = f"Lidl {slug}"
            else:
                title = raw_title.get_text(strip=True) if raw_title else "Lidl √öjs√°g"
            
            if link not in seen:
                status = analyze_link("Lidl", title, link)
                if status == "KEEP":
                    print(f"[{status}] {title} -> {link}")
                    # VALIDITY T√ñR√ñLVE
                    found.append({"store": "Lidl", "title": title, "url": link})
                seen.add(link)
    except Exception as e:
        print(f"‚ùå Lidl Hiba: {e}")
    return found


def scan_tesco():
    print("\n--- TESCO Szkennel√©s ---")
    url = "https://www.tesco.hu/akciok/katalogusok/"
    seen = set()
    found = []
    try:
        response = cffi_requests.get(url, impersonate="chrome110", headers={'User-Agent': 'Mozilla/5.0'}, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        for link in soup.find_all('a', href=True):
            href = link['href']
            if 'tesco-ujsag' in href and ('hipermarket' in href or 'szupermarket' in href):
                full_url = href if href.startswith('http') else f"https://www.tesco.hu{href}"
                
                # --- √öJ: TESCO URL ALAP√ö C√çMGENER√ÅL√ÅS ---
                match = re.search(r'/katalogusok/([^/]+/[^/]+)', full_url)
                if match:
                    title = f"Tesco {match.group(1)}"
                else:
                    title = "Tesco Hipermarket" if "hipermarket" in href else "Tesco Szupermarket"
                
                if full_url not in seen:
                    status = analyze_link("Tesco", title, full_url)
                    if status == "KEEP":
                        print(f"[{status}] {title} -> {full_url}")
                        # VALIDITY T√ñR√ñLVE
                        found.append({"store": "Tesco", "title": title, "url": full_url})
                    seen.add(full_url)
    except Exception as e:
        print(f"‚ùå Tesco Hiba: {e}")
    return found


def scan_aldi():
    print("\n--- ALDI Szkennel√©s ---")
    url = "https://www.aldi.hu/hu/ajanlatok/online-akcios-ujsag.html"
    found = []
    try:
        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
        soup = BeautifulSoup(response.text, 'html.parser')
        seen = set()
        for a in soup.find_all('a', href=True):
            if 'szorolap.aldi.hu' in a['href']:
                href = a['href']
                title = a.get('title', 'Aldi √öjs√°g')
                if href not in seen:
                    status = analyze_link("Aldi", title, href)
                    if status == "KEEP":
                        # --- √öJ: C√çM CSER√âJE A LINK V√âG√âRE (SLUG) ---
                        better_title = get_slug_title("Aldi", title, href)
                        
                        print(f"[{status}] {better_title} -> {href}")
                        # VALIDITY T√ñR√ñLVE
                        found.append({"store": "Aldi", "title": better_title, "url": href})
                    seen.add(href)
    except:
        pass
    return found


# ===============================================================================
# --- √öJ: CBA & PR√çMA PDF VAD√ÅSZ MODUL (H√ÅL√ìZATI LEHALLGAT√ìVAL) ---
# ===============================================================================

def _hunt_cba_prima_pdfs(url, store_name):
    """Bels≈ë seg√©df√ºggv√©ny: Let√∂lti a nyers h√°l√≥zati PDF linkeket a CBA/Pr√≠ma weblapj√°r√≥l."""
    print(f"\nüöÄ {store_name} H√°l√≥zati PDF Vad√°szat Indul: {url}")
    
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--window-size=1920,1080")
    # A --disable-gpu √©s az egyedi User-Agent T√ñR√ñLVE, mert meg√∂lik a 3D FlipBook motorj√°t!
    options.add_argument("--no-sandbox") # Ez kell a GitHub Actions miatt
    options.add_argument("--disable-dev-shm-usage") # Ez is kell a GitHub Actions miatt
    options.set_capability("goog:loggingPrefs", {"performance": "ALL"})
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    pdf_links = set()
    
    try:
        driver.get(url)
        print("‚è≥ 1. Alap weboldal bet√∂lt√©se (5 mp)...")
        time.sleep(5)
        
        try:
            gombok = driver.find_elements(By.TAG_NAME, "button")
            for btn in gombok:
                txt = btn.text.lower()
                if "√∂sszes" in txt or "elfogad" in txt or "mindent" in txt:
                    driver.execute_script("arguments[0].click();", btn)
                    print("üç™ S√ºti ablak elt√°vol√≠tva.")
                    time.sleep(2)
                    break
        except Exception as e:
            print(f"‚ö†Ô∏è S√ºti hiba (nem gond): {e}")

        print("üìú 2. √öjs√°gok keres√©se √©s 'fel√©breszt√©se'...")
        flipbooks = driver.find_elements(By.CSS_SELECTOR, "._3d-flip-book")
        
        if flipbooks:
            print(f"üéØ Tal√°lt flipbook modulok sz√°ma: {len(flipbooks)} db")
            for i, fb in enumerate(flipbooks):
                try:
                    driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", fb)
                    time.sleep(1)
                    driver.execute_script("arguments[0].click();", fb)
                    print(f"   üëÜ {i+1}. √∫js√°g megkattintva. V√°rakoz√°s a h√°l√≥zati forgalomra...")
                    time.sleep(4)
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Nem siker√ºlt a(z) {i+1}. √∫js√°got fel√©breszteni: {e}")
        else:
            print("‚ùå Nem tal√°ltam 3D Flipbook elemet az oldalon!")
        
        print("‚è≥ 3. Utols√≥ v√°rakoz√°s a let√∂lt√©sek befejez√©s√©re (10 mp)...")
        time.sleep(10)

        print("üì° 4. H√°l√≥zati napl√≥ (Network log) elemz√©se...")
        logs = driver.get_log("performance")
        for entry in logs:
            log_message = entry.get("message")
            if log_message:
                try:
                    log_data = json.loads(log_message)
                    message = log_data.get("message", {})
                    method = message.get("method", "")
                    
                    if method in ["Network.requestWillBeSent", "Network.responseReceived"]:
                        params = message.get("params", {})
                        req_url = params.get("request", {}).get("url", "")
                        res_url = params.get("response", {}).get("url", "")
                        
                        for u in [req_url, res_url]:
                            if u and ".pdf" in u.lower():
                                pdf_links.add(u)
                except:
                    pass
                    
        if pdf_links:
             print(f"üéâ SIKER! {len(pdf_links)} db PDF linket tal√°ltunk a h√°l√≥zaton!")
        else:
             print("‚ùå √úres k√©zzel t√©rt√ºnk vissza, nincs PDF a napl√≥ban.")
                    
    except Exception as e:
        print(f"‚ùå V√©gzetes hiba a(z) {store_name} PDF vad√°szatn√°l: {e}")
    finally:
        driver.quit()
        
    return pdf_links


def scan_cba_combined():
    print("\n--- CBA / Pr√≠ma Szkennel√©s (PDF Vad√°sz + Automata D√°tumsz≈±r≈ë) ---")
    found = []
    today = datetime.date.today()
    
    targets = [
        ("CBA", "https://cba.hu/aktualis-ajanlataink/"),
        ("CBA Pr√≠ma", "https://prima.hu/aktualis-ajanlataink/")
    ]
    
    for store_name, url in targets:
        pdfs = _hunt_cba_prima_pdfs(url, store_name)
        
        for pdf_url in sorted(pdfs):
            title = pdf_url.split('/')[-1] 
            is_expired = False
            
            date_match = re.search(r'-(\d{2})(\d{2})\.pdf', pdf_url, re.IGNORECASE)
            if date_match:
                month = int(date_match.group(1))
                day = int(date_match.group(2))
                year = today.year
                
                year_match = re.search(r'/(\d{4})/\d{2}/', pdf_url)
                if year_match:
                    year = int(year_match.group(1))
                    
                try:
                    end_date = datetime.date(year, month, day)
                    if end_date < today:
                        is_expired = True
                except:
                    pass 
            
            if is_expired:
                print(f"[DROP] Lej√°rt PDF eldobva: {title}")
            else:
                print(f"[KEEP] {store_name} ({title}) -> {pdf_url}")
                found.append({
                    "store": store_name,
                    "title": title,
                    "url": pdf_url
                })
                
    return found

# =============================================================================
# 2. R√âSZ: COOP MISSZI√ì (Selenium)
# =============================================================================

def fresh_start(driver, wait):
    """√öjrat√∂lti az oldalt, hogy tiszta lappal induljon (Aktu√°lis f√ºl)."""
    print("\nüîÑ Oldal √∫jrat√∂lt√©se (Clean State)...")
    driver.get("https://www.coop.hu/ajanlatkereso/")
    time.sleep(3)

    # S√ºti kezel√©se minden egyes √∫jrat√∂lt√©sn√©l
    try:
        wait.until(EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), '√ñsszes s√ºti')]"))).click()
        print("üç™ S√ºtik t√∂r√∂lve.")
    except:
        # Ha nincs gomb, biztosra megy√ºnk a JS t√∂rl√©ssel
        driver.execute_script("document.querySelectorAll('.cookie-bar, #cookie-consent').forEach(el => el.remove());")

    # Megnyitjuk a boltv√°laszt√≥t
    wait.until(EC.element_to_be_clickable((By.XPATH, "//*[contains(text(), 'V√°lasszon Coop √ºzletet')]"))).click()
    time.sleep(1)


# 1. SZOLNOK (Dupla k√∂r: Szolnok ABC + H√≠d ABC)
def scan_szolnok(driver, wait, results):
    print("üìç SZOLNOK BEVET√âS INDUL...")
    fresh_start(driver, wait)

    # 1. BOLT: 170.SZ.SZOLNOK (Csak Aktu√°lis)
    ActionChains(driver).send_keys(Keys.TAB).perform()
    time.sleep(0.5)
    driver.switch_to.active_element.send_keys("5000" + Keys.ENTER)
    time.sleep(5)

    target_1 = "170.SZ.SZOLNOK"
    print(f"üîé [1/2] Bolt: {target_1}")
    bolt1 = wait.until(EC.element_to_be_clickable((By.XPATH, f"//*[contains(text(), '{target_1}')]")))
    driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", bolt1)
    time.sleep(1)
    driver.execute_script("arguments[0].click();", bolt1)
    time.sleep(6)

    driver.execute_script("window.scrollBy(0, 700);")
    time.sleep(3)

    print("üéØ Szolnok ABC: Aktu√°lis...")
    driver.execute_script("document.elementFromPoint(400, 500).click();")
    time.sleep(6)

    iframes = driver.find_elements(By.TAG_NAME, "iframe")
    for f in iframes:
        src = f.get_attribute("src")
        if src and "katalogus" in src:
            results["szolnok_abc"]["aktualis_link"] = src
            break

    ActionChains(driver).send_keys(Keys.ESCAPE).perform()

    # --- √öJRAIND√çT√ÅS A M√ÅSODIK BOLT EL≈êTT ---
    fresh_start(driver, wait)

    # 2. BOLT: H√çD ABC (Aktu√°lis + J√∂v≈ë heti)
    ActionChains(driver).send_keys(Keys.TAB).perform()
    time.sleep(0.5)
    driver.switch_to.active_element.send_keys("5000" + Keys.ENTER)
    time.sleep(5)

    target_2 = "H√çD ABC"
    print(f"üîé [2/2] Bolt: {target_2}")
    bolt2 = wait.until(EC.element_to_be_clickable((By.XPATH, f"//*[contains(text(), '{target_2}')]")))
    driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", bolt2)
    time.sleep(1)
    driver.execute_script("arguments[0].click();", bolt2)
    time.sleep(6)

    driver.execute_script("window.scrollBy(0, 700);")
    time.sleep(3)

    print("üéØ H√≠d ABC: Aktu√°lis...")
    driver.execute_script("document.elementFromPoint(400, 500).click();")
    time.sleep(6)

    iframes = driver.find_elements(By.TAG_NAME, "iframe")
    for f in iframes:
        src = f.get_attribute("src")
        if src and "katalogus" in src:
            results["hid_abc"]["aktualis_link"] = src
            break

    ActionChains(driver).send_keys(Keys.ESCAPE).perform()
    time.sleep(4)

    print("üéØ H√≠d ABC: J√∂v≈ë heti...")
    driver.execute_script("document.elementFromPoint(825, 294).click();")  # Gomb
    time.sleep(4)
    driver.execute_script("document.elementFromPoint(825, 600).click();")  # √öjs√°g
    time.sleep(6)

    iframes = driver.find_elements(By.TAG_NAME, "iframe")
    for f in iframes:
        src = f.get_attribute("src")
        if src and "katalogus" in src:
            results["hid_abc"]["jovoheti_link"] = src
            break

    ActionChains(driver).send_keys(Keys.ESCAPE).perform()


# 2. KECSKEM√âT
def scan_kecskemet(driver, wait, results):
    print("üìç KECSKEM√âT BEVET√âS INDUL...")
    fresh_start(driver, wait)

    ActionChains(driver).send_keys(Keys.TAB).perform()
    time.sleep(0.5)
    driver.switch_to.active_element.send_keys("6000" + Keys.ENTER)
    time.sleep(5)

    print("üîé Bolt: SZ√âCHENYIV√ÅROSI...")
    bolt = wait.until(EC.element_to_be_clickable((By.XPATH, "//*[contains(text(), 'SZ√âCHENYIV√ÅROSI')]")))
    driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", bolt)
    time.sleep(1)
    driver.execute_script("arguments[0].click();", bolt)
    time.sleep(6)

    driver.execute_script("window.scrollBy(0, 700);")
    time.sleep(3)

    print("üéØ Aktu√°lis...")
    driver.execute_script("document.elementFromPoint(400, 500).click();")
    time.sleep(6)

    iframes = driver.find_elements(By.TAG_NAME, "iframe")
    for f in iframes:
        src = f.get_attribute("src")
        if src and "katalogus" in src:
            results["kecskemet"]["aktualis_link"] = src
            break

    ActionChains(driver).send_keys(Keys.ESCAPE).perform()
    time.sleep(4)

    print("üéØ J√∂v≈ë heti...")
    driver.execute_script("document.elementFromPoint(825, 294).click();")
    time.sleep(4)
    driver.execute_script("document.elementFromPoint(825, 600).click();")
    time.sleep(6)

    iframes = driver.find_elements(By.TAG_NAME, "iframe")
    for f in iframes:
        src = f.get_attribute("src")
        if src and "katalogus" in src:
            results["kecskemet"]["jovoheti_link"] = src
            break

    ActionChains(driver).send_keys(Keys.ESCAPE).perform()


# 3. DEBRECEN
def scan_debrecen(driver, wait, results):
    print("üìç DEBRECEN BEVET√âS INDUL...")
    fresh_start(driver, wait)

    ActionChains(driver).send_keys(Keys.TAB).perform()
    time.sleep(0.5)
    driver.switch_to.active_element.send_keys("4032" + Keys.ENTER)
    time.sleep(5)

    target = "51. SZ. √âLELMISZERBOLT"
    print(f"üîé Bolt: {target}")
    bolt = wait.until(EC.element_to_be_clickable((By.XPATH, f"//*[contains(text(), '{target}')]")))
    driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", bolt)
    time.sleep(1)
    driver.execute_script("arguments[0].click();", bolt)
    time.sleep(6)

    driver.execute_script("window.scrollBy(0, 700);")
    time.sleep(3)

    print("üéØ Aktu√°lis...")
    driver.execute_script("document.elementFromPoint(400, 500).click();")
    time.sleep(6)

    iframes = driver.find_elements(By.TAG_NAME, "iframe")
    for f in iframes:
        src = f.get_attribute("src")
        if src and "katalogus" in src:
            results["debrecen"]["aktualis_link"] = src
            break

    ActionChains(driver).send_keys(Keys.ESCAPE).perform()
    time.sleep(4)

    print("üéØ J√∂v≈ë heti...")
    driver.execute_script("document.elementFromPoint(825, 294).click();")
    time.sleep(4)
    driver.execute_script("document.elementFromPoint(825, 600).click();")
    time.sleep(6)

    iframes = driver.find_elements(By.TAG_NAME, "iframe")
    for f in iframes:
        src = f.get_attribute("src")
        if src and "katalogus" in src:
            results["debrecen"]["jovoheti_link"] = src
            break

    ActionChains(driver).send_keys(Keys.ESCAPE).perform()


# 4. P√âCS
def scan_pecs(driver, wait, results):
    print("üìç P√âCS BEVET√âS INDUL...")
    fresh_start(driver, wait)

    ActionChains(driver).send_keys(Keys.TAB).perform()
    time.sleep(0.5)
    driver.switch_to.active_element.send_keys("7623" + Keys.ENTER)
    time.sleep(5)

    target = "240 COOP ABC P√âCS"
    print(f"üîé Bolt: {target}")
    bolt = wait.until(EC.element_to_be_clickable((By.XPATH, f"//*[contains(text(), '{target}')]")))
    driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", bolt)
    time.sleep(1)
    driver.execute_script("arguments[0].click();", bolt)
    time.sleep(6)

    driver.execute_script("window.scrollBy(0, 700);")
    time.sleep(3)

    print("üéØ Aktu√°lis...")
    driver.execute_script("document.elementFromPoint(400, 500).click();")
    time.sleep(6)

    iframes = driver.find_elements(By.TAG_NAME, "iframe")
    for f in iframes:
        src = f.get_attribute("src")
        if src and "katalogus" in src:
            results["pecs"]["aktualis_link"] = src
            break

    ActionChains(driver).send_keys(Keys.ESCAPE).perform()
    time.sleep(4)

    print("üéØ J√∂v≈ë heti...")
    driver.execute_script("document.elementFromPoint(825, 294).click();")
    time.sleep(4)
    driver.execute_script("document.elementFromPoint(825, 600).click();")
    time.sleep(6)

    iframes = driver.find_elements(By.TAG_NAME, "iframe")
    for f in iframes:
        src = f.get_attribute("src")
        if src and "katalogus" in src:
            results["pecs"]["jovoheti_link"] = src
            break

    ActionChains(driver).send_keys(Keys.ESCAPE).perform()


# 5. SZOMBATHELY (HERM√ÅN ABC - JAV√çTOTT TENYEREL√âSSEL)
def scan_szombathely(driver, wait, results):
    print("üìç SZOMBATHELY BEVET√âS INDUL...")
    fresh_start(driver, wait)

    ActionChains(driver).send_keys(Keys.TAB).perform()
    time.sleep(0.5)
    driver.switch_to.active_element.send_keys("9700" + Keys.ENTER)
    time.sleep(5)

    target = "HERM√ÅN ABC"
    print(f"üîé Bolt: {target}")
    bolt = wait.until(EC.element_to_be_clickable((By.XPATH, f"//*[contains(text(), '{target}')]")))
    driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", bolt)
    time.sleep(1)
    driver.execute_script("arguments[0].click();", bolt)
    time.sleep(6)

    driver.execute_script("window.scrollBy(0, 700);")
    time.sleep(3)

    # --- JAV√çTVA: Y=126 helyett Y=500 (√öjs√°g k√∂zepe) ---
    print("üéØ Aktu√°lis (X:493, Y:500 - TENYEREL√âS)...")
    driver.execute_script("document.elementFromPoint(493, 500).click();")
    time.sleep(6)

    iframes = driver.find_elements(By.TAG_NAME, "iframe")
    for f in iframes:
        src = f.get_attribute("src")
        if src and "katalogus" in src:
            results["szombathely"]["aktualis_link"] = src
            break

    ActionChains(driver).send_keys(Keys.ESCAPE).perform()
    time.sleep(4)

    # --- JAV√çTVA: Gomb: Y=126, √öjs√°g: Y=500 ---
    print("üéØ J√∂v≈ë heti (Gomb: X:823 Y:126 | √öjs√°g: X:823 Y:500)...")
    driver.execute_script("document.elementFromPoint(823, 126).click();")  # F√úL KIV√ÅLASZT√ÅSA
    time.sleep(4)
    driver.execute_script("document.elementFromPoint(823, 500).click();")  # TENYEREL√âS AZ √öJS√ÅGRA
    time.sleep(6)

    iframes = driver.find_elements(By.TAG_NAME, "iframe")
    for f in iframes:
        src = f.get_attribute("src")
        if src and "katalogus" in src:
            results["szombathely"]["jovoheti_link"] = src
            break

    ActionChains(driver).send_keys(Keys.ESCAPE).perform()


# ===============================================================================
# F≈êVEZ√âRL≈ê (EGYES√çTETT)
# ===============================================================================

def main():
    print("=== MASTER SCANNER: FLAYER SCANNER + COOP ALL-IN-ONE ===")

    all_flyers = []

    # --- 1. HAGYOM√ÅNYOS BOLTOK ---
    # !!! FONTOS: SPAR KER√úLT EL≈êRE !!!
    # Most a k√ºls≈ë modult h√≠vja meg
    all_flyers.extend(scan_spar())
    
    all_flyers.extend(scan_penny())
    all_flyers.extend(scan_lidl())
    all_flyers.extend(scan_metro())
    all_flyers.extend(scan_tesco())
    all_flyers.extend(scan_auchan())
    all_flyers.extend(scan_aldi())
    all_flyers.extend(scan_cba_combined())

    # --- 2. COOP MISSZI√ì (Selenium) ---
    print("\nüöÄ COOP MISSZI√ì INDUL (H√ÅTT√âRBEN/HEADLESS)...")

    coop_results = {
        "szolnok_abc": {"aktualis_link": None},
        "hid_abc": {"aktualis_link": None, "jovoheti_link": None},
        "kecskemet": {"aktualis_link": None, "jovoheti_link": None},
        "debrecen": {"aktualis_link": None, "jovoheti_link": None},
        "pecs": {"aktualis_link": None, "jovoheti_link": None},
        "szombathely": {"aktualis_link": None, "jovoheti_link": None}
    }

    opts = Options()
    opts.add_argument("--window-size=1920,1080")

    # --- FEJLESZT√âS: H√ÅTT√âRBEN FUTTAT√ÅS (HEADLESS) AKTIV√ÅLVA ---
    opts.add_argument("--headless")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)
    wait = WebDriverWait(driver, 15)

    try:
        scan_szolnok(driver, wait, coop_results)
        scan_kecskemet(driver, wait, coop_results)
        scan_debrecen(driver, wait, coop_results)
        scan_pecs(driver, wait, coop_results)
        scan_szombathely(driver, wait, coop_results)
    except Exception as e:
        print(f"‚ùå Coop Hiba: {e}")
    finally:
        driver.quit()

    # --- 3. COOP EREDM√âNYEK HOZZ√ÅAD√ÅSA A K√ñZ√ñS LIST√ÅHOZ ---
    
    for key, links in coop_results.items():
        # URL alap√∫ n√©vmeghat√°roz√°s a k√©rt t√©rk√©p szerint
        url_to_check = links.get("aktualis_link") or links.get("jovoheti_link") or ""
        url_lower = url_to_check.lower()
        
        store_display_name = f"Coop {key}" # Alap√©rtelmezett

        # T√âRK√âP ALKALMAZ√ÅSA
        if "mecsek" in url_lower:
            store_display_name = "Coop Mecsek F√ºsz√©rt"
        elif "tisza" in url_lower or "szolnok" in url_lower:
            store_display_name = "Tisza-Coop"
        elif "alfold" in url_lower or "alf√∂ld" in url_lower or "kecskemet" in url_lower:
            store_display_name = "Alf√∂ld Pro-Coop"
        # --- JAV√çT√ÅS: ITT A K√âRT M√ìDOS√çT√ÅS ---
        elif "hetforras" in url_lower or "h√©tforr√°s" in url_lower or "szombathely" in url_lower:
            store_display_name = "Coop H√©tforr√°s"
        elif "eszak-kelet" in url_lower or "debrecen" in url_lower or "miskolc" in url_lower:
            store_display_name = "√âszak-Kelet Pro-Coop"
        elif "honi" in url_lower:
            store_display_name = "Honi-Coop"
        elif "polus" in url_lower or "p√≥lus" in url_lower:
            store_display_name = "P√≥lus-Coop"

        # TISZT√çT√ì SZAB√ÅLY (Zrt, Kft gyilkos)
        for bad_suffix in ["Zrt.", "Zrt", "Kft.", "Kft", "Kereskedelmi"]:
            store_display_name = store_display_name.replace(bad_suffix, "").strip()

        # --- √öJ: C√çM CSER√âJE COOP-N√ÅL IS (URL SLUG ALAPJ√ÅN) ---
        if links.get("aktualis_link"):
            url = links["aktualis_link"]
            # Kiv√°gjuk a link utols√≥ r√©sz√©t (pl. coop-tisza-szorolap-2026-februar-3-het-szuper-plusz)
            # Figyelem: ha a link '/' jellel v√©gz≈ëdik, az utols√≥ elem √ºres lehet, ez√©rt rstrip kell
            slug = url.rstrip('/').split('/')[-1]
            if slug:
                final_title = slug
            else:
                final_title = "Aktu√°lis"

            all_flyers.append({
                "store": store_display_name,
                "title": final_title,
                "url": url
                # VALIDITY T√ñR√ñLVE
            })
            print(f"[COOP] {store_display_name} ({final_title}) hozz√°adva.")

        if links.get("jovoheti_link"):
            url = links["jovoheti_link"]
            slug = url.rstrip('/').split('/')[-1]
            if slug:
                final_title = slug
            else:
                final_title = "J√∂v≈ë heti"

            all_flyers.append({
                "store": store_display_name,
                "title": final_title,
                "url": url
                # VALIDITY T√ñR√ñLVE
            })
            print(f"[COOP] {store_display_name} ({final_title}) hozz√°adva.")

    # --- 4. MENT√âS ---
    final_json = {
        "last_updated": str(datetime.datetime.now()),
        "flyers": all_flyers
    }

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_json, f, ensure_ascii=False, indent=4)

    print(f"\nüíæ SIKER! √ñsszesen {len(all_flyers)} db √∫js√°g linkje (Hagyom√°nyos + Coop + SPAR) mentve ide: {OUTPUT_FILE}")
    print("Most ellen≈ërizd a JSON-t, √©s ha j√≥, ind√≠tsd a Feldolgoz√≥ Robotot!")


if __name__ == "__main__":
    main()



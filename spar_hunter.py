import json
import re
import datetime
from curl_cffi import requests
from bs4 import BeautifulSoup

# --- KONFIGUR√ÅCI√ì ---
OUTPUT_FILE = 'spar_flyers.json'


def scan_spar_only():
    print("=== üéØ SPAR LINKVAD√ÅSZ (C√©lzott Keres√©s) ===")
    url = "https://www.spar.hu/ajanlatok"

    # Er≈ës b√∂ng√©sz≈ë √°lc√°z√°s (Anti-Bot v√©delem ellen)
    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'accept-language': 'hu-HU,hu;q=0.9,en-US;q=0.8,en;q=0.7',
        'cache-control': 'max-age=0',
        'upgrade-insecure-requests': '1',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'
    }

    found_flyers = []

    try:
        print(f"üì° Kapcsol√≥d√°s: {url} ...")
        response = requests.get(url, impersonate="chrome124", headers=headers, timeout=20)

        if response.status_code != 200:
            print(f"‚ùå HIBA: A szerver {response.status_code} k√≥ddal v√°laszolt!")
            return []

        soup = BeautifulSoup(response.text, 'html.parser')
        links = soup.find_all('a', href=True)
        print(f"üîé Tal√°lt linkek sz√°ma: {len(links)} db")

        seen_urls = set()

        # Id≈ëkapu: Csak az elm√∫lt 30 nap (√©s j√∂v≈ëbeli) √∫js√°gok kellenek
        today = datetime.date.today()
        cutoff_date = today - datetime.timedelta(days=30)

        for a in links:
            raw_href = a['href']

            # --- 1. SZ≈∞R≈ê: √ârdekes lehet ez a link? ---
            # Keres√ºnk kulcsszavakat: spar, interspar, ajanlatok, szorolap
            is_interesting = False
            if 'spar' in raw_href.lower() and ('ajanlatok' in raw_href.lower() or 'szorolap' in raw_href.lower()):
                is_interesting = True

            if not is_interesting:
                continue

            # PDF √©s egy√©b szemetek kiz√°r√°sa
            if "getPdf" in raw_href or ".pdf" in raw_href or "ViewPdf" in raw_href:
                continue

            # --- 2. LINK NORMALIZ√ÅL√ÅS (A diagnosztika alapj√°n!) ---
            # Ha relat√≠v link (pl. /ajanlatok/spar/...), kieg√©sz√≠tj√ºk
            full_url = raw_href
            if raw_href.startswith('/'):
                full_url = f"https://www.spar.hu{raw_href}"

            if full_url in seen_urls:
                continue

            # --- 3. D√ÅTUM KINYER√âSE (YYMMDD form√°tum) ---
            # Keress√ºk a 6 jegy≈± sz√°mot, ami d√°tumnak n√©z ki (pl. 260212)
            # A regex: 2 sz√°m (√©v) + 2 sz√°m (h√≥nap) + 2 sz√°m (nap)
            date_match = re.search(r'(2[4-6])(0[1-9]|1[0-2])(0[1-9]|[12]\d|3[01])', full_url)

            validity_str = "Keres√©s..."

            if date_match:
                y_str, m_str, d_str = date_match.groups()
                try:
                    # D√°tum valid√°l√°s
                    year = 2000 + int(y_str)
                    month = int(m_str)
                    day = int(d_str)

                    flyer_date = datetime.date(year, month, day)

                    # Ha t√∫l r√©gi, eldobjuk
                    if flyer_date < cutoff_date:
                        # print(f"   -> T√∫l r√©gi: {flyer_date}")
                        continue

                    # Sz√°molunk egy √©rv√©nyess√©gi id≈ët (Start + 6 nap)
                    end_date = flyer_date + datetime.timedelta(days=6)
                    validity_str = f"{flyer_date.strftime('%Y.%m.%d')}-{end_date.strftime('%Y.%m.%d')}"

                except ValueError:
                    continue  # Nem val√≥s d√°tum
            else:
                # Ha nincs d√°tum a linkben, lehet, hogy gy≈±jt≈ëoldal -> kihagyjuk, vagy kock√°ztatunk
                # A diagnosztika alapj√°n a relev√°ns linkekben MINDIG volt sz√°m (260212)
                continue

                # --- 4. C√çM GENER√ÅL√ÅS ---
            title = "SPAR √öjs√°g"
            if "interspar" in full_url.lower():
                title = "INTERSPAR"
            elif "spar-market" in full_url.lower():
                title = "SPAR market"
            elif "spar-extra" in full_url.lower():
                title = "SPAR Partner (Extra)"

            # --- TAL√ÅLAT! ---
            print(f"‚úÖ TAL√ÅLAT: {title} | {validity_str} | {full_url}")

            found_flyers.append({
                "store": "Spar",
                "title": title,
                "url": full_url,
                "validity": validity_str
            })
            seen_urls.add(full_url)

    except Exception as e:
        print(f"‚ùå KRITIKUS HIBA: {e}")

    # --- MENT√âS ---
    if found_flyers:
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump({"flyers": found_flyers}, f, ensure_ascii=False, indent=4)
        print(f"\nüíæ SIKER! {len(found_flyers)} db SPAR √∫js√°g mentve ide: {OUTPUT_FILE}")
    else:
        print("\n‚ö†Ô∏è NEM TAL√ÅLTAM √öJS√ÅGOT. (Ellen≈ërizd, hogy nem blokkoltak-e)")
    
    # KIEG√âSZ√çT√âS: Visszaadjuk a list√°t a Master Scannernek!
    return found_flyers


if __name__ == "__main__":
    scan_spar_only()
